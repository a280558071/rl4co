{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with unbatchable environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While all the environments in RL4CO are batched, it is not common to find CO problems that are hard (or impossible) to be written in a *batchable* way.\n",
    "\n",
    "To overcome this issue, one might want to code the environment in an *unbatched* way, meaning that the logic coded in the environment is responsible to `reset`, `step`, compute the reward and render a **single** problem instance at time.\n",
    "Then, one would like to run this unbatched environment in parallel, in order to speed up the data collection.\n",
    "\n",
    "Fortunately, this can be easily done using TorchRL's features, and this tutorial will show you how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbatched TSP\n",
    "To simplify the understanding, we decide to use a simple environment like the Travelling Salesman Problem (TSP).\n",
    "\n",
    "We start by importing the needed packages. This step is similar to the one for batched environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "from tensordict.tensordict import TensorDict\n",
    "from torchrl.data import (\n",
    "    BoundedTensorSpec,\n",
    "    CompositeSpec,\n",
    "    UnboundedContinuousTensorSpec,\n",
    "    UnboundedDiscreteTensorSpec,\n",
    ")\n",
    "\n",
    "from rl4co.envs.common.base import RL4COEnvBase\n",
    "from rl4co.utils.ops import gather_by_index, get_tour_length\n",
    "from rl4co.utils.pylogger import get_pylogger\n",
    "\n",
    "from rl4co.envs.routing.tsp.generator import TSPGenerator\n",
    "from rl4co.envs.routing.tsp.render import render\n",
    "\n",
    "log = get_pylogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the environment class, that inherits from the `RL4COEnvBase`. The definition of the environment is very similar to the batched one, but all the tensors have batch size 1.\n",
    "\n",
    "Since we want to parallelize the environment using TorchRL, it is very important to define the environment specs.\n",
    "\n",
    "Another important thing to notice is that, since the `reset` method of the env calls the `generator` object with `self.batch_size`, we need to set this attribute to `[1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnbatchedTSPEnv(RL4COEnvBase):\n",
    "    \"\"\"Traveling Salesman Problem (TSP) environment\n",
    "    At each step, the agent chooses a city to visit. The reward is 0 unless the agent visits all the cities.\n",
    "    In that case, the reward is (-)length of the path: maximizing the reward is equivalent to minimizing the path length.\n",
    "\n",
    "    Observations:\n",
    "        - locations of each customer.\n",
    "        - the current location of the vehicle.\n",
    "\n",
    "    Constrains:\n",
    "        - the tour must return to the starting customer.\n",
    "        - each customer must be visited exactly once.\n",
    "\n",
    "    Finish condition:\n",
    "        - the agent has visited all customers and returned to the starting customer.\n",
    "\n",
    "    Reward:\n",
    "        - (minus) the negative length of the path.\n",
    "\n",
    "    Args:\n",
    "        generator: TSPGenerator instance as the data generator\n",
    "        generator_params: parameters for the generator\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"tsp\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator: TSPGenerator = None,\n",
    "        generator_params: dict = {},\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        if generator is None:\n",
    "            generator = TSPGenerator(**generator_params)\n",
    "        self.generator = generator\n",
    "        self.batch_size = [1] # needed for the reset method, that calls the generator using the self.batch_size\n",
    "        self._make_spec(self.generator)\n",
    "\n",
    "    @staticmethod\n",
    "    def _step(td: TensorDict) -> TensorDict:\n",
    "        current_node = td[\"action\"]\n",
    "        first_node = current_node if td[\"i\"].all() == 0 else td[\"first_node\"]\n",
    "\n",
    "        # # Set not visited to 0 (i.e., we visited the node)\n",
    "        available = td[\"action_mask\"].scatter(\n",
    "            -1, current_node.unsqueeze(-1).expand_as(td[\"action_mask\"]), 0\n",
    "        )\n",
    "\n",
    "        # We are done there are no unvisited locations\n",
    "        done = torch.sum(available, dim=-1) == 0\n",
    "\n",
    "        # The reward is calculated outside via get_reward for efficiency, so we set it to 0 here\n",
    "        reward = torch.zeros_like(done)\n",
    "\n",
    "        td.update(\n",
    "            {\n",
    "                \"first_node\": first_node,\n",
    "                \"current_node\": current_node,\n",
    "                \"i\": td[\"i\"] + 1,\n",
    "                \"action_mask\": available,\n",
    "                \"reward\": reward,\n",
    "                \"done\": done,\n",
    "            },\n",
    "        )\n",
    "        return td\n",
    "\n",
    "    def _reset(self, td: Optional[TensorDict] = None, batch_size=None) -> TensorDict:\n",
    "        # Initialize locations\n",
    "        device = td.device\n",
    "        init_locs = td[\"locs\"]\n",
    "\n",
    "        # We do not enforce loading from self for flexibility\n",
    "        num_loc = init_locs.shape[-2]\n",
    "\n",
    "        # Other variables\n",
    "        current_node = torch.zeros((1,), dtype=torch.int64, device=device)\n",
    "        available = torch.ones(\n",
    "            (1, num_loc), dtype=torch.bool, device=device\n",
    "        )  # 1 means not visited, i.e. action is allowed\n",
    "        i = torch.zeros((1, 1), dtype=torch.int64, device=device)\n",
    "\n",
    "        return TensorDict(\n",
    "            {\n",
    "                \"locs\": init_locs,\n",
    "                \"first_node\": current_node,\n",
    "                \"current_node\": current_node,\n",
    "                \"i\": i,\n",
    "                \"action_mask\": available,\n",
    "                \"reward\": torch.zeros((1, 1), dtype=torch.float32),\n",
    "            },\n",
    "            batch_size=1,\n",
    "        )\n",
    "\n",
    "    def _make_spec(self, generator: TSPGenerator):\n",
    "        self.observation_spec = CompositeSpec(\n",
    "            locs=BoundedTensorSpec(\n",
    "                low=generator.min_loc,\n",
    "                high=generator.max_loc,\n",
    "                shape=(1, generator.num_loc, 2),\n",
    "                dtype=torch.float32,\n",
    "            ),\n",
    "            first_node=UnboundedDiscreteTensorSpec(\n",
    "                shape=(1,),\n",
    "                dtype=torch.int64,\n",
    "            ),\n",
    "            current_node=UnboundedDiscreteTensorSpec(\n",
    "                shape=(1,),\n",
    "                dtype=torch.int64,\n",
    "            ),\n",
    "            i=UnboundedDiscreteTensorSpec(\n",
    "                shape=(1,),\n",
    "                dtype=torch.int64,\n",
    "            ),\n",
    "            action_mask=UnboundedDiscreteTensorSpec(\n",
    "                shape=(1, generator.num_loc),\n",
    "                dtype=torch.bool,\n",
    "            ),\n",
    "            shape=(1,),\n",
    "        )\n",
    "        self.action_spec = BoundedTensorSpec(\n",
    "            shape=(1,),\n",
    "            dtype=torch.int64,\n",
    "            low=0,\n",
    "            high=generator.num_loc,\n",
    "        )\n",
    "        self.reward_spec = UnboundedContinuousTensorSpec(shape=(1))\n",
    "        self.done_spec = UnboundedDiscreteTensorSpec(shape=(1), dtype=torch.bool)\n",
    "\n",
    "    def _get_reward(self, td, actions) -> TensorDict:\n",
    "        if self.check_solution:\n",
    "            self.check_solution_validity(td, actions)\n",
    "\n",
    "        # Gather locations in order of tour and return distance between them (i.e., -reward)\n",
    "        locs_ordered = gather_by_index(td[\"locs\"], actions)\n",
    "        return -get_tour_length(locs_ordered)\n",
    "\n",
    "    @staticmethod\n",
    "    def check_solution_validity(td: TensorDict, actions: torch.Tensor):\n",
    "        \"\"\"Check that solution is valid: nodes are visited exactly once\"\"\"\n",
    "        assert (\n",
    "            torch.arange(actions.size(1), out=actions.data.new())\n",
    "            .view(1, -1)\n",
    "            .expand_as(actions)\n",
    "            == actions.data.sort(1)[0]\n",
    "        ).all(), \"Invalid tour\"\n",
    "\n",
    "    @staticmethod\n",
    "    def render(td: TensorDict, actions: torch.Tensor=None, ax = None):\n",
    "        return render(td, actions, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that our environment works properly by testing the `reset` and `step` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset td:\n",
      " TensorDict(\n",
      "    fields={\n",
      "        action_mask: Tensor(shape=torch.Size([1, 20]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        current_node: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        first_node: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        i: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        locs: Tensor(shape=torch.Size([1, 20, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        reward: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([1]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "\n",
      "Step td:\n",
      " {'next': TensorDict(\n",
      "    fields={\n",
      "        action: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        action_mask: Tensor(shape=torch.Size([1, 20]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        current_node: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        first_node: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        i: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "        locs: Tensor(shape=torch.Size([1, 20, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([1]),\n",
      "    device=None,\n",
      "    is_shared=False)}\n"
     ]
    }
   ],
   "source": [
    "tsp_env = UnbatchedTSPEnv()\n",
    "td = tsp_env.reset()\n",
    "\n",
    "print(\"Reset td:\\n\", td)\n",
    "\n",
    "td[\"action\"] = torch.tensor([0])\n",
    "td = tsp_env.step(td)\n",
    "print(\"\\nStep td:\\n\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check that the action was correctly performed by checking the `action_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action mask:\\n\", td[\"next\"][\"action_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the first node was correctly masked out. As a final check, let's see what happens if we visit all the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = td[\"next\"]\n",
    "num_locs = td[\"locs\"].shape[-2]\n",
    "for i in range(1, num_locs): # we have already visited the first node\n",
    "    td[\"action\"] = torch.tensor([i])\n",
    "    td = tsp_env.step(td)\n",
    "    td = td[\"next\"]\n",
    "\n",
    "print(\"Done:\\n\", td[\"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the episode reached status `done` as we would expect. Let's check if the reward can be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = torch.arange(num_locs, device=td.device).unsqueeze(0)\n",
    "reward = tsp_env.get_reward(td, actions)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the environment is working properly. Let's make it parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelizing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To parallelize the environment, we will use the `ParallelEnv` from TorchRL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs.batched_envs import ParallelEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, following TorchRL best practices, we check if we defined the environment properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute '_get_str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchrl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_env_specs\n\u001b[0;32m----> 2\u001b[0m \u001b[43mcheck_env_specs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtsp_env\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Git/rl4co/.venv/lib/python3.10/site-packages/torchrl/envs/utils.py:729\u001b[0m, in \u001b[0;36mcheck_env_specs\u001b[0;34m(env, return_contiguous, check_dtype, seed)\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m check_env_specs(\n\u001b[1;32m    725\u001b[0m             env, return_contiguous\u001b[38;5;241m=\u001b[39mreturn_contiguous, check_dtype\u001b[38;5;241m=\u001b[39mcheck_dtype\n\u001b[1;32m    726\u001b[0m         )\n\u001b[1;32m    728\u001b[0m fake_tensordict \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mfake_tensordict()\n\u001b[0;32m--> 729\u001b[0m real_tensordict \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_contiguous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_contiguous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_contiguous:\n\u001b[1;32m    732\u001b[0m     fake_tensordict \u001b[38;5;241m=\u001b[39m fake_tensordict\u001b[38;5;241m.\u001b[39munsqueeze(real_tensordict\u001b[38;5;241m.\u001b[39mbatch_dims \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Git/rl4co/.venv/lib/python3.10/site-packages/torchrl/envs/common.py:2562\u001b[0m, in \u001b[0;36mEnvBase.rollout\u001b[0;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, return_contiguous, tensordict, set_truncated, out)\u001b[0m\n\u001b[1;32m   2552\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensordict\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensordict,\n\u001b[1;32m   2554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_cast_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m: auto_cast_to_device,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m: callback,\n\u001b[1;32m   2560\u001b[0m }\n\u001b[1;32m   2561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m break_when_any_done:\n\u001b[0;32m-> 2562\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rollout_stop_early\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2564\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rollout_nonstop(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Git/rl4co/.venv/lib/python3.10/site-packages/torchrl/envs/common.py:2645\u001b[0m, in \u001b[0;36mEnvBase._rollout_stop_early\u001b[0;34m(self, tensordict, auto_cast_to_device, max_steps, policy, policy_device, env_device, callback)\u001b[0m\n\u001b[1;32m   2642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m max_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2643\u001b[0m     \u001b[38;5;66;03m# we don't truncate as one could potentially continue the run\u001b[39;00m\n\u001b[1;32m   2644\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 2645\u001b[0m tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_mdp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[38;5;66;03m# done and truncated are in done_keys\u001b[39;00m\n\u001b[1;32m   2648\u001b[0m \u001b[38;5;66;03m# We read if any key is done.\u001b[39;00m\n\u001b[1;32m   2649\u001b[0m any_done \u001b[38;5;241m=\u001b[39m _terminated_or_truncated(\n\u001b[1;32m   2650\u001b[0m     tensordict,\n\u001b[1;32m   2651\u001b[0m     full_done_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_spec[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_done_spec\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   2652\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2653\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Git/rl4co/.venv/lib/python3.10/site-packages/torchrl/envs/utils.py:297\u001b[0m, in \u001b[0;36m_StepMDP.__call__\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    291\u001b[0m     out \u001b[38;5;241m=\u001b[39m LazyStackedTensorDict\u001b[38;5;241m.\u001b[39mlazy_stack(\n\u001b[1;32m    292\u001b[0m         [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(td) \u001b[38;5;28;01mfor\u001b[39;00m td \u001b[38;5;129;01min\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mtensordicts],\n\u001b[1;32m    293\u001b[0m         tensordict\u001b[38;5;241m.\u001b[39mstack_dim,\n\u001b[1;32m    294\u001b[0m     )\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 297\u001b[0m next_td \u001b[38;5;241m=\u001b[39m \u001b[43mtensordict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_str\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate(tensordict):\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_other:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute '_get_str'"
     ]
    }
   ],
   "source": [
    "from torchrl.envs import check_env_specs\n",
    "check_env_specs(tsp_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For clarity, we decide to only use 2 parallel envs, but you can set a higher number if your hardware is happy about it.\n",
    "\n",
    "The `ParallelEnv` class requires 2 arguments: the number of parallel environments and a callable that returns an environment. We will provide a *lambda function* that creates it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = ParallelEnv(2, lambda: UnbatchedTSPEnv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the same checks we did on the unbatched environment. We start checking if the `reset` and `step` methods work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = envs.reset()\n",
    "print(\"Reset td:\\n\", td)\n",
    "\n",
    "td[\"action\"] = torch.zeros((2, 1), dtype=torch.int64)\n",
    "td = envs.step(td)\n",
    "print(\"\\nStep td:\\n\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that now the batch size is 2 and that all the shapes are correct.\n",
    "We proceed with the action mask check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action mask:\\n\", td[\"next\"][\"action_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, in both environments the first entry in the mask is correctly set.\n",
    "Finally, let's see if we can run an entire episode and compute the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = td[\"next\"]\n",
    "num_locs = td[\"locs\"].shape[-2]\n",
    "for i in range(1, num_locs): # we have already visited the first node\n",
    "    td[\"action\"] = torch.tensor([[i, i]])\n",
    "    td = tsp_env.step(td)\n",
    "    td = td[\"next\"]\n",
    "\n",
    "print(\"Done:\\n\", td[\"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = torch.arange(num_locs, device=td.device).repeat(2, 1)\n",
    "reward = tsp_env.get_reward(td, actions)\n",
    "print(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
