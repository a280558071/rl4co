{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0185840f-e92c-456c-a6c3-26c4c3eeecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor \n",
    "from torch.nn.modules import Module\n",
    "from einops import rearrange\n",
    "from rl4co.utils.ops import batchify, unbatchify\n",
    "from rl4co.models.zoo.common.autoregressive.decoder import PrecomputedCache\n",
    "from rl4co.envs import RL4COEnvBase\n",
    "from tensordict import TensorDict\n",
    "from rl4co.envs import RL4COEnvBase\n",
    "from rl4co.models.nn.env_embeddings import env_init_embedding\n",
    "from rl4co.models.nn.attention import MultiHeadCrossAttention\n",
    "from rl4co.models.zoo.matnet.encoder import MixedScoresSDPA\n",
    "from rl4co.envs.scheduling.jssp import JSSPEnv\n",
    "from rl4co.models.nn.ops import Normalization\n",
    "from rl4co.models import DecoderOnlyPolicy, AutoregressivePolicy, L2DReinforce, AttentionModel, AutoregressiveDecoder\n",
    "from rl4co.models.nn.graph.gcn import GCNEncoder\n",
    "from rl4co.utils.ops import adj_to_pyg_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ede4d4a-001c-41a1-b09c-f1287213a1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not set a default GPU\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    def get_free_gpu():\n",
    "        import subprocess\n",
    "        from io import StringIO\n",
    "        import pandas as pd\n",
    "\n",
    "        gpu_stats = subprocess.check_output([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=memory.used,memory.free\"])\n",
    "        gpu_df = pd.read_csv(StringIO(gpu_stats.decode(\"utf-8\")),\n",
    "                            names=['memory.used', 'memory.free'],\n",
    "                            skiprows=1)\n",
    "        print('GPU usage:\\n{}'.format(gpu_df))\n",
    "        gpu_df['memory.free'] = gpu_df['memory.free'].map(lambda x: x.rstrip(' [MiB]'))\n",
    "        idx = gpu_df['memory.free'].idxmax()\n",
    "        print('Returning GPU{} with {} free MiB'.format(idx, gpu_df.iloc[idx]['memory.free']))\n",
    "        return idx\n",
    "\n",
    "    free_gpu_id = get_free_gpu()\n",
    "    torch.cuda.set_device(free_gpu_id)\n",
    "    print(torch.cuda.is_available())\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.current_device())\n",
    "\n",
    "except:\n",
    "    \n",
    "    print(\"Could not set a default GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd705ffb-a519-41bc-b280-41723dc4d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(f\"cuda:{torch.cuda.current_device()}\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d848703-78a0-4d32-a1c4-d48368909cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = JSSPEnv(6, 6, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a6b9c2f-8155-4799-8b88-5b803ce38dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = env._reset(batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0e7f904-21cf-4a3e-9555-6a9aa693d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderBlock(nn.Module):\n",
    "\n",
    "#     def __init__(\n",
    "#         self, \n",
    "#         embedding_dim=128, \n",
    "#         num_heads=8, \n",
    "#         num_scores=1,\n",
    "#         feed_forward_hidden=256,\n",
    "#         normalization=\"batch\",\n",
    "#     ):\n",
    "#         super(EncoderBlock, self).__init__()\n",
    "#         ms = MixedScoresSDPA(num_heads, num_scores=num_scores)\n",
    "#         self.cross_attn_block = MultiHeadCrossAttention(\n",
    "#             embedding_dim, num_heads, sdpa_fn=ms\n",
    "#         )\n",
    "#         self.F_a = nn.ModuleDict(\n",
    "#             {\n",
    "#                 \"norm1\": Normalization(embedding_dim, normalization),\n",
    "#                 \"ffn\": nn.Sequential(\n",
    "#                     nn.Linear(embedding_dim, feed_forward_hidden),\n",
    "#                     nn.ReLU(),\n",
    "#                     nn.Linear(feed_forward_hidden, embedding_dim),\n",
    "#                 ),\n",
    "#                 \"norm2\": Normalization(embedding_dim, normalization),\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, dmat=None, mask=None):\n",
    "#         x_out = self.cross_attn_block(x, x, cross_attn_mask=mask, dmat=dmat)\n",
    "#         x_emb_out = self.F_a[\"norm1\"](x + x_out)\n",
    "#         x_emb_out = self.F_a[\"norm2\"](x_emb_out + self.F_a[\"ffn\"](x_emb_out))\n",
    "#         return x_emb_out\n",
    "\n",
    "\n",
    "# class AttnEncoder(nn.Module):\n",
    "\n",
    "#     def __init__(self, embedding_dim=128, num_heads=8, num_layers=3, linear_bias=False):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.init_emb = JSSPInitEmbedding(embedding_dim)\n",
    "#         self.layers = nn.ModuleList([\n",
    "#             EncoderBlock(embedding_dim, num_heads, num_scores=1) for _ in range(num_layers)\n",
    "#         ])\n",
    "    \n",
    "#     def forward(self, td):\n",
    "#         bs, num_jobs, num_ops = td[\"durations\"].shape\n",
    "        \n",
    "#         init_emb = self.init_emb(td)\n",
    "#         op_emb = init_emb.clone()\n",
    "#         # dmat = torch.stack(tuple(td.select(\"ops_on_same_ma_adj\", \"adjacency\").values()), dim=-1)\n",
    "#         dmat = td[\"adjacency\"]\n",
    "        \n",
    "#         for layer in self.layers:\n",
    "#             op_emb = layer(op_emb, dmat=dmat)\n",
    "        \n",
    "#         job_init_emb = init_emb.gather(1, td[\"next_op\"][...,None].expand(bs, num_jobs, self.embedding_dim))\n",
    "#         job_emb = op_emb.gather(1, td[\"next_op\"][...,None].expand(bs, num_jobs, self.embedding_dim))\n",
    "        \n",
    "#         return job_emb, job_init_emb\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_dim=128, \n",
    "        num_heads=8, \n",
    "        num_scores=1,\n",
    "        feed_forward_hidden=256,\n",
    "        normalization=\"batch\",\n",
    "    ):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        ms = MixedScoresSDPA(num_heads, num_scores=num_scores)\n",
    "        self.cross_attn_block = MultiHeadCrossAttention(\n",
    "            embedding_dim, num_heads, sdpa_fn=ms\n",
    "        )\n",
    "        self.F_a = nn.ModuleDict(\n",
    "            {\n",
    "                \"norm1\": Normalization(embedding_dim, normalization),\n",
    "                \"ffn\": nn.Sequential(\n",
    "                    nn.Linear(embedding_dim, feed_forward_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(feed_forward_hidden, embedding_dim),\n",
    "                ),\n",
    "                \"norm2\": Normalization(embedding_dim, normalization),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def forward(self, x, dmat=None, mask=None):\n",
    "        x_out = self.cross_attn_block(x, x, cross_attn_mask=mask, dmat=dmat)\n",
    "        x_emb_out = self.F_a[\"norm1\"](x + x_out)\n",
    "        x_emb_out = self.F_a[\"norm2\"](x_emb_out + self.F_a[\"ffn\"](x_emb_out))\n",
    "        return x_emb_out\n",
    "\n",
    "\n",
    "class AttnEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            env_name,\n",
    "            embedding_dim=128, \n",
    "            num_heads=8, \n",
    "            num_layers=3, \n",
    "            init_embedding: nn.Module = None,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        if isinstance(env_name, RL4COEnvBase):\n",
    "            env_name = env_name.name\n",
    "        self.env_name = env_name\n",
    "\n",
    "        self.init_embedding = (\n",
    "            env_init_embedding(self.env_name, {\"embedding_dim\": embedding_dim})\n",
    "            if init_embedding is None\n",
    "            else init_embedding\n",
    "        )\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(embedding_dim, num_heads, num_scores=2) for _ in range(num_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, td):\n",
    "        bs, num_jobs, num_ops = td[\"durations\"].shape\n",
    "        \n",
    "        init_emb = self.init_embedding(td)\n",
    "        op_emb = init_emb.clone()\n",
    "        dmat = torch.stack(\n",
    "            tuple(td.select(\"ops_on_same_ma_adj\", \"ops_of_same_job\").values()), \n",
    "            dim=-1\n",
    "        )\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            op_emb = layer(op_emb, dmat=dmat)\n",
    "        \n",
    "        # job_init_emb = init_emb.gather(1, td[\"next_op\"][...,None].expand(bs, num_jobs, self.embedding_dim))\n",
    "        # job_emb = op_emb.gather(1, td[\"next_op\"][...,None].expand(bs, num_jobs, self.embedding_dim))\n",
    "        \n",
    "        return op_emb, None\n",
    "\n",
    "\n",
    "class AttnDecoder(AutoregressiveDecoder):\n",
    "    def __init__(\n",
    "            self, \n",
    "            env_name, \n",
    "            embedding_dim: int, \n",
    "            num_heads: int, \n",
    "            use_graph_context: bool = True, \n",
    "            linear_bias: bool = False, \n",
    "            context_embedding: Module = None, \n",
    "            dynamic_embedding: Module = None, \n",
    "            **logit_attn_kwargs\n",
    "        ):\n",
    "        super().__init__(\n",
    "            env_name, \n",
    "            embedding_dim, \n",
    "            num_heads, \n",
    "            use_graph_context, \n",
    "            linear_bias, \n",
    "            context_embedding, \n",
    "            dynamic_embedding, \n",
    "            **logit_attn_kwargs\n",
    "        )\n",
    "\n",
    "    def _get_log_p(\n",
    "        self,\n",
    "        cached: PrecomputedCache,\n",
    "        td: TensorDict,\n",
    "        softmax_temp: float = None,\n",
    "        num_starts: int = 0,\n",
    "    ):\n",
    "        next_op = td[\"next_op\"][..., None].expand(-1, -1, self.embedding_dim)\n",
    "        # Get precomputed (cached) embeddings\n",
    "        node_embeds_cache, graph_context_cache = (\n",
    "            cached.node_embeddings.gather(1, next_op),\n",
    "            cached.graph_context,\n",
    "        )\n",
    "        glimpse_k_stat, glimpse_v_stat, logit_k_stat = (\n",
    "            cached.glimpse_key.gather(1, next_op),\n",
    "            cached.glimpse_val.gather(1, next_op),\n",
    "            cached.logit_key.gather(1, next_op),\n",
    "        )  # [B, N, H]\n",
    "        has_dyn_emb_multi_start = self.is_dynamic_embedding and num_starts > 1\n",
    "\n",
    "        # Handle efficient multi-start decoding\n",
    "        if has_dyn_emb_multi_start:\n",
    "            # if num_starts > 0 and we have some dynamic embeddings, we need to reshape them to [B*S, ...]\n",
    "            # since keys and values are not shared across starts (i.e. the episodes modify these embeddings at each step)\n",
    "            glimpse_k_stat = batchify(glimpse_k_stat, num_starts)\n",
    "            glimpse_v_stat = batchify(glimpse_v_stat, num_starts)\n",
    "            logit_k_stat = batchify(logit_k_stat, num_starts)\n",
    "            node_embeds_cache = batchify(node_embeds_cache, num_starts)\n",
    "            graph_context_cache = (\n",
    "                batchify(graph_context_cache, num_starts)\n",
    "                if isinstance(graph_context_cache, Tensor)\n",
    "                else graph_context_cache\n",
    "            )\n",
    "        elif num_starts > 1:\n",
    "            td = unbatchify(td, num_starts)\n",
    "            if isinstance(graph_context_cache, Tensor):\n",
    "                # add a dimension for num_starts (will automatically be broadcasted during addition)\n",
    "                graph_context_cache = graph_context_cache.unsqueeze(1)\n",
    "\n",
    "        step_context = self.context_embedding(cached.node_embeddings, td)\n",
    "        glimpse_q = step_context + graph_context_cache\n",
    "        glimpse_q = (\n",
    "            glimpse_q.unsqueeze(1) if glimpse_q.ndim == 2 else glimpse_q\n",
    "        )  # add seq_len dim if not present\n",
    "\n",
    "        # Compute dynamic embeddings and add to static embeddings\n",
    "        glimpse_k_dyn, glimpse_v_dyn, logit_k_dyn = self.dynamic_embedding(td)\n",
    "        glimpse_k = glimpse_k_stat + glimpse_k_dyn\n",
    "        glimpse_v = glimpse_v_stat + glimpse_v_dyn\n",
    "        logit_k = logit_k_stat + logit_k_dyn\n",
    "\n",
    "        # Get the mask\n",
    "        mask = ~td[\"action_mask\"]\n",
    "\n",
    "        # Compute logits\n",
    "        log_p = self.logit_attention(\n",
    "            glimpse_q, glimpse_k, glimpse_v, logit_k, mask, softmax_temp\n",
    "        )\n",
    "\n",
    "        # Now we need to reshape the logits and log_p to [B*S,N,...] is num_starts > 1 without dynamic embeddings\n",
    "        # note that rearranging order is important here\n",
    "        if num_starts > 1 and not has_dyn_emb_multi_start:\n",
    "            log_p = rearrange(log_p, \"b s l -> (s b) l\", s=num_starts)\n",
    "            mask = rearrange(mask, \"b s l -> (s b) l\", s=num_starts)\n",
    "        return log_p, mask\n",
    "\n",
    "      \n",
    "class L2DEncoder(GCNEncoder):\n",
    "\n",
    "        \n",
    "    def __init__(self, embedding_dim, num_layers):\n",
    "    \n",
    "        def edge_idx_fn(td, _):\n",
    "            return adj_to_pyg_edge_index(td[\"adjacency\"])\n",
    "        \n",
    "        super().__init__(\"jssp\", embedding_dim, num_layers, edge_idx_fn=edge_idx_fn)\n",
    "\n",
    "    def forward(self, td):\n",
    "        bs, num_jobs, num_ops = td[\"durations\"].shape\n",
    "        op_emb, init_emb = super().forward(td)\n",
    "        job_init_emb = init_emb.gather(1, td[\"next_op\"][...,None].expand(bs, num_jobs, self.embedding_dim))\n",
    "        job_emb = op_emb.gather(1, td[\"next_op\"][...,None].expand(bs, num_jobs, self.embedding_dim))\n",
    "        \n",
    "        return job_emb, job_init_emb     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e07cf05-ade8-4c04-a072-46fd3331b132",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 128\n",
    "layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0211422b-bd8a-443e-b43a-dd8d620e1421",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2d_enc = L2DEncoder(emb_dim, layers)\n",
    "attn_enc = AttnEncoder(\"jssp\", emb_dim, num_layers=layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35211692",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_dec = AttnDecoder(env_name=\"jssp\", embedding_dim=emb_dim, num_heads=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8acd1c2c-21f8-4df1-a63c-0a760296ff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 128])\n",
      "torch.Size([10, 36, 128])\n"
     ]
    }
   ],
   "source": [
    "l2d_emb, _ = l2d_enc(td)\n",
    "attn_emb, _ = attn_enc(td)\n",
    "print(l2d_emb.shape)\n",
    "print(attn_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0396413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = attn_dec(td, attn_emb, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88209756-f1c0-4364-83b8-30f79a2a7fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2d_policy = DecoderOnlyPolicy(\n",
    "    env_name=env, \n",
    "    embedding_dim=emb_dim, \n",
    "    feature_extractor=l2d_enc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c45a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_policy = AutoregressivePolicy(\n",
    "    env_name=\"jssp\",\n",
    "    encoder=attn_enc, \n",
    "    decoder=attn_dec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfc2512c-cfb6-4c4e-9c7e-49e2c06eb334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luttmann/opt/miniconda3/envs/rl4co/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'env' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['env'])`.\n",
      "/Users/luttmann/opt/miniconda3/envs/rl4co/lib/python3.9/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'policy' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['policy'])`.\n"
     ]
    }
   ],
   "source": [
    "l2d_model = L2DReinforce(\n",
    "    env, \n",
    "    l2d_policy,\n",
    "    batch_size = 100,\n",
    "    val_batch_size = None,\n",
    "    test_batch_size = None,\n",
    "    train_data_size = 10000,\n",
    "    val_data_size = 1000,\n",
    "    test_data_size = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d90a1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_model = AttentionModel(\n",
    "    env, \n",
    "    attn_policy,\n",
    "    batch_size = 100,\n",
    "    val_batch_size = None,\n",
    "    test_batch_size = None,\n",
    "    train_data_size = 10000,\n",
    "    val_data_size = 1000,\n",
    "    test_data_size = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8543da0-5186-4c32-a3b0-90afdd0abcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luttmann/opt/miniconda3/envs/rl4co/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:551: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/luttmann/opt/miniconda3/envs/rl4co/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "from rl4co.utils.trainer import RL4COTrainer\n",
    "\n",
    "trainer = RL4COTrainer(\n",
    "    max_epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52ca9bf3-e07e-4eb3-bb7e-761e52538d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val_file not set. Generating dataset instead\n",
      "test_file not set. Generating dataset instead\n",
      "\n",
      "  | Name     | Type                 | Params\n",
      "--------------------------------------------------\n",
      "0 | env      | JSSPEnv              | 0     \n",
      "1 | policy   | AutoregressivePolicy | 499 K \n",
      "2 | baseline | WarmupBaseline       | 499 K \n",
      "--------------------------------------------------\n",
      "998 K     Trainable params\n",
      "0         Non-trainable params\n",
      "998 K     Total params\n",
      "3.995     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685d67c6cdf843bca35dd2364ec6731e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luttmann/opt/miniconda3/envs/rl4co/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/luttmann/opt/miniconda3/envs/rl4co/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d29c0ceae5e1499db5a9a515f582876d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luttmann/opt/miniconda3/envs/rl4co/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(attn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa51f5-e436-42e0-a4ee-8725ea547821",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(l2d_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
